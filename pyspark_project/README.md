# ğŸ¥ Healthcare Claims Analytics with PySpark & GCP

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)](https://spark.apache.org/)
[![GCP](https://img.shields.io/badge/GCP-BigQuery-4285F4.svg)](https://cloud.google.com/bigquery)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Production-grade big data analytics pipeline for healthcare claims and patient data using **PySpark** on **Google Cloud Platform** with **Synthea synthetic medical data**.

## ğŸ¯ Project Overview

This project demonstrates distributed data processing and machine learning on large-scale healthcare data:

- **Dataset**: Synthea Synthetic Patient Data (10M+ records, realistic medical claims)
- **Platform**: Google Cloud Platform (BigQuery, Cloud Storage, Dataproc)
- **Framework**: PySpark for distributed processing (handles TB-scale data)
- **Use Cases**: Patient readmission prediction, cost analysis, treatment pattern mining

## ğŸ“Š Dataset: Synthea Synthetic Medical Data

**What is Synthea?**
- Realistic synthetic patient records (HIPAA-compliant, no PHI)
- Generated using medical guidelines and statistical models
- Includes: demographics, encounters, conditions, medications, procedures, claims

**Available on GCP Public Datasets:**
- `bigquery-public-data.cms_synthetic_patient_data_omop` (OMOP CDM format)
- Includes 1M+ synthetic patients
- ~10GB compressed, 100GB+ uncompressed
- Perfect for PySpark distributed processing

**Data Schema:**
```
â”œâ”€â”€ patients         # Demographics, birth, death dates
â”œâ”€â”€ encounters       # Hospital visits, ER, outpatient
â”œâ”€â”€ conditions       # Diagnoses (ICD-10 codes)
â”œâ”€â”€ procedures       # Surgeries, treatments (CPT codes)
â”œâ”€â”€ medications      # Prescriptions (RxNorm codes)
â”œâ”€â”€ observations     # Labs, vitals, measurements
â””â”€â”€ claims           # Billing, insurance, costs
```

## ğŸš€ Key Features

âœ… **Distributed Processing** - PySpark on Google Dataproc clusters  
âœ… **Big Data Pipeline** - ETL with 100GB+ synthetic medical records  
âœ… **ML at Scale** - MLlib for distributed machine learning  
âœ… **Cost Optimization** - Spot instances, auto-scaling clusters  
âœ… **Production Ready** - Logging, monitoring, error handling  
âœ… **GCP Integration** - BigQuery, Cloud Storage, Dataproc

## ğŸ“ Project Structure

```
pyspark_project/
â”‚
â”œâ”€â”€ data/                          # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw Synthea files (CSV/Parquet)
â”‚   â”œâ”€â”€ processed/                 # Cleaned/transformed data
â”‚   â””â”€â”€ external/                  # Reference data (ICD codes, drug lists)
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_modeling.ipynb
â”‚   â””â”€â”€ 04_results_analysis.ipynb
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data_ingestion/           # Load data from GCS/BigQuery
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bigquery_loader.py
â”‚   â”‚   â””â”€â”€ gcs_loader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_processing/          # ETL and data cleaning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cleaner.py
â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                 # Feature engineering
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ patient_features.py
â”‚   â”‚   â”œâ”€â”€ encounter_features.py
â”‚   â”‚   â””â”€â”€ temporal_features.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # ML models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ readmission_predictor.py
â”‚   â”‚   â”œâ”€â”€ cost_predictor.py
â”‚   â”‚   â””â”€â”€ risk_stratification.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark_session.py
â”‚   â”‚   â”œâ”€â”€ gcp_utils.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚
â”‚   â””â”€â”€ visualization/            # Plotting and dashboards
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ plots.py
â”‚       â””â”€â”€ dashboard.py
â”‚
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ spark_config.yaml
â”‚   â”œâ”€â”€ gcp_config.yaml
â”‚   â””â”€â”€ model_config.yaml
â”‚
â”œâ”€â”€ scripts/                       # Executable scripts
â”‚   â”œâ”€â”€ setup_gcp.sh
â”‚   â”œâ”€â”€ submit_spark_job.sh
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ batch_predict.py
â”‚
â”œâ”€â”€ tests/                         # Unit tests
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â””â”€â”€ test_models.py
â”‚
â”œâ”€â”€ models/                        # Saved models
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ outputs/                       # Results
â”‚   â”œâ”€â”€ figures/                  # Plots and visualizations
â”‚   â”œâ”€â”€ reports/                  # Analysis reports
â”‚   â””â”€â”€ logs/                     # Application logs
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â””â”€â”€ api_reference.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

## ğŸ› ï¸ Technologies

| Component | Technology |
|-----------|-----------|
| **Big Data Framework** | Apache Spark 3.5+ |
| **Language** | Python 3.8+ |
| **Cloud Platform** | Google Cloud Platform |
| **Data Storage** | Google Cloud Storage, BigQuery |
| **Compute** | Google Dataproc (managed Spark) |
| **ML Library** | PySpark MLlib |
| **Orchestration** | Apache Airflow / Cloud Composer |
| **Visualization** | Matplotlib, Seaborn, Plotly |
| **Notebooks** | Jupyter, Google Colab |

## ğŸ“‹ Prerequisites

### Local Development
```bash
python >= 3.8
pyspark >= 3.5.0
jupyter
pandas, numpy, matplotlib
```

### Google Cloud Platform
```bash
# Install Google Cloud SDK
curl https://sdk.cloud.google.com | bash

# Initialize and authenticate
gcloud init
gcloud auth login
gcloud auth application-default login

# Set project
gcloud config set project YOUR_PROJECT_ID
```

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone <your-repo-url>
cd pyspark_project
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Set Up GCP
```bash
# Create GCS bucket
gsutil mb gs://your-healthcare-data-bucket

# Enable required APIs
gcloud services enable dataproc.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
```

### 4. Download Synthea Data
```python
# Option 1: Use GCP Public Dataset (BigQuery)
from google.cloud import bigquery

client = bigquery.Client(project='your-project-id')
query = """
SELECT *
FROM `bigquery-public-data.cms_synthetic_patient_data_omop.person`
LIMIT 1000
"""
df = client.query(query).to_dataframe()

# Option 2: Generate locally with Synthea
# Download from: https://synthetichealth.github.io/synthea/
java -jar synthea-with-dependencies.jar -p 10000
```

### 5. Run PySpark Job Locally
```bash
# Test locally
python scripts/train_model.py --mode local

# Submit to Dataproc
./scripts/submit_spark_job.sh
```

### 6. Launch Jupyter Notebook
```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

## ğŸ“Š Use Cases

### 1. **Hospital Readmission Prediction**
- Predict 30-day readmission risk using patient history
- Features: diagnoses, procedures, medications, demographics
- Model: PySpark Random Forest Classifier
- Metric: AUC-ROC, Precision/Recall

### 2. **Healthcare Cost Forecasting**
- Predict total claim costs per patient
- Features: chronic conditions, utilization patterns, age
- Model: PySpark Linear Regression / Gradient Boosted Trees
- Metric: RMSE, MAE, RÂ²

### 3. **Patient Risk Stratification**
- Segment patients into high/medium/low risk groups
- Features: comorbidities, ER visits, medication adherence
- Model: K-Means clustering
- Output: Risk scores for care management

### 4. **Treatment Pattern Mining**
- Discover common treatment pathways using association rules
- Technique: FP-Growth algorithm
- Output: Frequent itemsets, treatment sequences

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Synthea Data  â”‚
â”‚  (BigQuery/GCS) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion â”‚ â† PySpark reads from BigQuery/GCS
â”‚   (PySpark)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Processing â”‚ â† ETL: clean, transform, join
â”‚   (PySpark DF)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Feature Engineer â”‚ â† Create ML features
â”‚  (PySpark SQL)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML Training    â”‚ â† MLlib: RF, GBT, LR
â”‚ (PySpark MLlib) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Serving  â”‚ â† Batch predictions
â”‚  (Dataproc)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Results/Viz    â”‚ â† Save to BigQuery/GCS
â”‚  (BigQuery)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Sample Results

*(To be added after model training)*

| Model | Use Case | AUC-ROC | Accuracy | Training Time |
|-------|----------|---------|----------|---------------|
| Random Forest | Readmission | 0.78 | 74% | 45 min (10 nodes) |
| GBT | Cost Prediction | - | RÂ²=0.65 | 1.2 hr (10 nodes) |
| K-Means | Risk Stratification | - | Silhouette=0.42 | 20 min (5 nodes) |

## ğŸ”§ Configuration

### Dataproc Cluster
```yaml
# config/spark_config.yaml
cluster:
  name: healthcare-spark-cluster
  region: us-central1
  master:
    machine_type: n1-standard-4
    disk_size: 100GB
  workers:
    count: 10
    machine_type: n1-standard-4
    disk_size: 100GB
  
spark:
  spark.executor.memory: 4g
  spark.executor.cores: 2
  spark.sql.shuffle.partitions: 200
```

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Test specific module
pytest tests/test_data_processing.py

# With coverage
pytest --cov=src tests/
```

## ğŸ“š Documentation

- **[Architecture Guide](docs/architecture.md)** - System design and data flow
- **[Data Dictionary](docs/data_dictionary.md)** - Schema and field descriptions
- **[API Reference](docs/api_reference.md)** - Function/class documentation

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Syntheaâ„¢** - Synthetic Patient Generator (https://synthetichealth.github.io/synthea/)
- **Apache Spark** - Distributed computing framework
- **Google Cloud Platform** - Cloud infrastructure
- **MIMIC Project** - Healthcare data standards inspiration

## ğŸ“ Contact

**Author**: [Your Name]  
**Email**: your.email@example.com  
**GitHub**: [@yourusername](https://github.com/yourusername)  
**LinkedIn**: [Your Profile](https://linkedin.com/in/yourprofile)

---

**Note**: This project uses synthetic medical data (Synthea) which does not contain any real patient information. All data is HIPAA-compliant and safe for public use.
