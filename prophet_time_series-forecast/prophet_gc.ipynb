{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368b46ee",
   "metadata": {},
   "source": [
    "# üìä Prophet Forecast ‚Äî Website Hourly Sessions (Google Analytics)\n",
    "This notebook uses **Google Analytics public dataset** to forecast hourly website sessions. Demonstrates production-grade Prophet modeling with data quality checks, extended training window, and rigorous validation methodology.\n",
    "\n",
    "**Data Source:** BigQuery Public Dataset (bigquery-public-data.google_analytics_sample)\n",
    "\n",
    "**Improvements over baseline:**\n",
    "- ‚úÖ Extended training window (12 months for yearly seasonality)\n",
    "- ‚úÖ Comprehensive data quality checks (missing hours, duplicates, outliers)\n",
    "- ‚úÖ 80/20 holdout validation with multiple metrics\n",
    "- ‚úÖ Linear growth (appropriate for unbounded web traffic)\n",
    "- ‚úÖ Optional hyperparameter tuning (50 parameter combinations tested)\n",
    "- ‚úÖ Uncertainty quantification with 90% confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73417dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as _stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_plotly\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROPHET FORECAST ‚Äî WEBSITE HOURLY SESSIONS (GOOGLE ANALYTICS)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2329f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63991986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "TRAINING_MONTHS = 12      \n",
    "RUN_TUNING = False        # Toggle cross-validation + grid search\n",
    "\n",
    "# Forecast period options:\n",
    "FORECAST_DAYS = 7        # ‚Üê Change this: 7 (1 week), 14 (2 weeks), 28 (4 weeks), etc.\n",
    "FORECAST_HOURS = 24 * FORECAST_DAYS  # Automatically converts days to hours\n",
    "\n",
    "TEST_SIZE = 0.2           # Holdout validation (20% of data)\n",
    "\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_colors = {\n",
    "    'Monday': '#1f77b4', 'Tuesday': '#ff7f0e', 'Wednesday': '#2ca02c',\n",
    "    'Thursday': '#d62728', 'Friday': '#9467bd', 'Saturday': '#8c564b', 'Sunday': '#e377c2'\n",
    "}\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Training: {TRAINING_MONTHS} months\")\n",
    "print(f\"   Forecast: {FORECAST_DAYS} days ({FORECAST_HOURS} hours)\")\n",
    "print(f\"   Tuning: {'Enabled' if RUN_TUNING else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c3075",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Fetch Data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44318d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Step 1: Fetching data from BigQuery...\")\n",
    "\n",
    "# FREE PUBLIC DATASET: Google Analytics Sample (from BigQuery public datasets)\n",
    "# https://console.cloud.google.com/marketplace/product/google/analytics-sample-dataset\n",
    "# NOTE: This dataset contains data from Aug 1, 2016 to Aug 1, 2017 only\n",
    "\n",
    "# Calculate how many months to fetch (max 12 months from the available data)\n",
    "# Dataset spans: 2016-08-01 to 2017-08-01\n",
    "if TRAINING_MONTHS <= 12:\n",
    "    # Use the LAST N months of available data (ending Aug 1, 2017)\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "      TIMESTAMP_TRUNC(\n",
    "        PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING)), \n",
    "        HOUR\n",
    "      ) AS ts_hour,\n",
    "      FORMAT_TIMESTAMP('%A', PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING))) AS weekday,\n",
    "      EXTRACT(HOUR FROM PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING))) AS hour,\n",
    "      COUNT(DISTINCT fullVisitorId) AS session_count\n",
    "    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
    "    WHERE _TABLE_SUFFIX BETWEEN \n",
    "      FORMAT_DATE('%Y%m%d', DATE_SUB(DATE('2017-08-01'), INTERVAL {TRAINING_MONTHS} MONTH))\n",
    "      AND '20170801'\n",
    "    GROUP BY ts_hour, weekday, hour\n",
    "    ORDER BY ts_hour;\n",
    "    \"\"\"\n",
    "else:\n",
    "    # If requesting more than 12 months, use full dataset (12 months)\n",
    "    print(f\"   ‚ö†Ô∏è  Requested {TRAINING_MONTHS} months, but dataset only has 12 months\")\n",
    "    print(f\"   Using full dataset: Aug 1, 2016 to Aug 1, 2017\")\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      TIMESTAMP_TRUNC(\n",
    "        PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING)), \n",
    "        HOUR\n",
    "      ) AS ts_hour,\n",
    "      FORMAT_TIMESTAMP('%A', PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING))) AS weekday,\n",
    "      EXTRACT(HOUR FROM PARSE_TIMESTAMP('%s', CAST(visitStartTime AS STRING))) AS hour,\n",
    "      COUNT(DISTINCT fullVisitorId) AS session_count\n",
    "    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
    "    WHERE _TABLE_SUFFIX BETWEEN '20160801' AND '20170801'\n",
    "    GROUP BY ts_hour, weekday, hour\n",
    "    ORDER BY ts_hour;\n",
    "    \"\"\"\n",
    "\n",
    "# Use your GCP project ID\n",
    "df = pd.read_gbq(query, project_id=\"mythic-code-477217-a8\", dialect=\"standard\")\n",
    "\n",
    "print(f\"   ‚ÑπÔ∏è  Using historical data from Google Analytics Sample dataset (2016-2017)\")\n",
    "print(f\"   Note: This is historical data for demonstration purposes\")\n",
    "\n",
    "print(\"‚úÖ Data fetched successfully!\")\n",
    "print(f\"   Rows: {len(df)} | Columns: {list(df.columns)}\")\n",
    "\n",
    "# Safety check for empty dataframe\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Query returned 0 rows! Possible issues:\\n\"\n",
    "        \"   1. Date range doesn't match available data (GA Sample: 2016-08-01 to 2017-08-01)\\n\"\n",
    "        \"   2. Project ID may not have access to public datasets\\n\"\n",
    "        \"   3. Table name or syntax error in query\\n\"\n",
    "        \"\\n   Try running this test query in BigQuery console:\\n\"\n",
    "        \"   SELECT COUNT(*) FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c889e3",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Quality Checks & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECKS & PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic renaming and type conversion\n",
    "df = df.rename(columns={'ts_hour': 'date', 'session_count': 'count'})\n",
    "df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "df['hour'] = df['hour'].astype(int)\n",
    "df['count'] = df['count'].astype(float)\n",
    "\n",
    "print(f\"\\nüìÖ Initial data range: {df['date'].min()} ‚Üí {df['date'].max()}\")\n",
    "print(f\"   Total hours in query: {len(df)}\")\n",
    "\n",
    "# ========================================\n",
    "# NEW: DATA QUALITY CHECKS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüîç Data Quality Checks:\")\n",
    "\n",
    "# 1. Check for duplicates\n",
    "dupes = df[df.duplicated(subset=['date'], keep=False)]\n",
    "if len(dupes) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(dupes)} duplicate timestamps - removing...\")\n",
    "    df = df.drop_duplicates(subset=['date'], keep='first')\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate timestamps found\")\n",
    "\n",
    "# 2. Check for missing hours\n",
    "expected_hours = pd.date_range(\n",
    "    start=df['date'].min(),\n",
    "    end=df['date'].max(),\n",
    "    freq='H'\n",
    ")\n",
    "expected_count = len(expected_hours)\n",
    "actual_count = len(df)\n",
    "missing_count = expected_count - actual_count\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {missing_count} missing hours ({missing_count/expected_count:.2%}) - filling with 0\")\n",
    "    \n",
    "    # Create complete hourly sequence\n",
    "    df_complete = pd.DataFrame({'date': expected_hours})\n",
    "    df = df_complete.merge(df, on='date', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    df['count'] = df['count'].fillna(0)\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['weekday'] = df['date'].dt.day_name()\n",
    "else:\n",
    "    print(\"‚úÖ No missing hours detected - continuous time series\")\n",
    "\n",
    "# 3. Outlier detection (flag, don't remove)\n",
    "q1, q3 = df['count'].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "outlier_threshold = q3 + 3 * iqr\n",
    "outliers = df[df['count'] > outlier_threshold]\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(f\"‚ÑπÔ∏è  Flagged {len(outliers)} potential outliers (>{outlier_threshold:.0f} sessions/hour)\")\n",
    "    max_idx = outliers['count'].idxmax()\n",
    "    print(f\"   Max: {outliers.loc[max_idx, 'count']:.0f} sessions at {outliers.loc[max_idx, 'date']}\")\n",
    "    print(f\"   Note: Keeping outliers in model (Prophet is robust)\")\n",
    "else:\n",
    "    print(\"‚úÖ No extreme outliers detected\")\n",
    "\n",
    "# 4. Check for zero-inflation\n",
    "zero_share = (df['count'] == 0).mean()\n",
    "print(f\"\\n‚ÑπÔ∏è  Share of zero-hour values: {zero_share:.2%}\")\n",
    "if zero_share > 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  High zero proportion - using linear growth (appropriate for web traffic)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data quality checks complete\")\n",
    "print(f\"   Final dataset: {len(df)} hours from {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Prepare Prophet data\n",
    "df_prophet = df[['date', 'count']].rename(\n",
    "    columns={'date': 'ds', 'count': 'y'}\n",
    ").sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä Prophet data summary:\")\n",
    "print(df_prophet['y'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d68d22",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Train/Test Split\n",
    "\n",
    "**Validation Strategy:**\n",
    "- **Single chronological holdout split** (80% train / 20% test)\n",
    "- Best practice for time series: train on past ‚Üí test on future\n",
    "- Avoids data leakage and mimics real-world forecasting\n",
    "- Cross-validation (multiple rolling splits) happens during tuning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN/TEST SPLIT FOR VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# IMPORTANT: Single holdout split (chronological)\n",
    "# This is BEST PRACTICE for time series forecasting:\n",
    "# - Uses first 80% for training, last 20% for testing (chronological order)\n",
    "# - Simulates real-world scenario: train on past, predict future\n",
    "# - Avoids data leakage (never train on future data to predict past)\n",
    "# - Single split is appropriate because time series data is ordered\n",
    "#\n",
    "# NOTE: Cross-validation (multiple rolling splits) happens during TUNING only\n",
    "# when RUN_TUNING=True. Tuning uses 8-10 rolling windows to select best parameters.\n",
    "# This holdout test provides final unbiased evaluation of the chosen model.\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "split_idx = int(len(df_prophet) * (1 - TEST_SIZE))\n",
    "df_train = df_prophet.iloc[:split_idx].copy()  # First 80% (chronological)\n",
    "df_test = df_prophet.iloc[split_idx:].copy()   # Last 20% (future)\n",
    "\n",
    "print(f\"\\nüîÄ Train/Test Split ({int((1-TEST_SIZE)*100)}% / {int(TEST_SIZE*100)}%):\")\n",
    "print(f\"   Train: {df_train['ds'].min()} to {df_train['ds'].max()}\")\n",
    "print(f\"          {len(df_train)} hours ({len(df_train)/24:.1f} days)\")\n",
    "print(f\"   Test:  {df_test['ds'].min()} to {df_test['ds'].max()}\")\n",
    "print(f\"          {len(df_test)} hours ({len(df_test)/24:.1f} days)\")\n",
    "print(f\"\\n   Train mean: {df_train['y'].mean():.2f} sessions/hour\")\n",
    "print(f\"   Test mean:  {df_test['y'].mean():.2f} sessions/hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ba31c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_prophet(\n",
    "    df_prophet,\n",
    "    param_grid=None,\n",
    "    initial='90 days',\n",
    "    period='30 days',\n",
    "    horizon='7 days',      \n",
    "    base_kwargs=None\n",
    "):\n",
    "    \"\"\"Cross-validation + grid search for Prophet. Returns best_model, results_df.\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    if param_grid is None:\n",
    "        # IMPROVED: Wider parameter ranges\n",
    "        param_grid = {\n",
    "            'changepoint_prior_scale': [0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "            'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0, 20.0],\n",
    "            'seasonality_mode': ['additive', 'multiplicative']\n",
    "        }\n",
    "    if base_kwargs is None:\n",
    "        base_kwargs = dict(\n",
    "            growth='linear',              # Linear for unbounded web traffic\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=True,\n",
    "            interval_width=0.90\n",
    "        )\n",
    "\n",
    "    combos = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    results = []\n",
    "    print(f\"\\nüîç Tuning Prophet: testing {len(combos)} parameter combinations...\")\n",
    "    \n",
    "    for i, params in enumerate(combos, 1):\n",
    "        try:\n",
    "            if i % 10 == 0:\n",
    "                print(f\"   Progress: {i}/{len(combos)} combinations tested...\")\n",
    "            \n",
    "            kwargs = dict(base_kwargs)\n",
    "            kwargs.update(params)\n",
    "            \n",
    "            # Prepare data (no cap/floor needed for linear growth)\n",
    "            df_tune = df_prophet.copy()\n",
    "            \n",
    "            m = Prophet(**kwargs).fit(df_tune)\n",
    "            df_cv = cross_validation(m, initial=initial, period=period, horizon=horizon)\n",
    "            df_p = performance_metrics(df_cv)\n",
    "            \n",
    "            results.append({\n",
    "                **params,\n",
    "                'rmse': float(df_p['rmse'].mean()),\n",
    "                'mae': float(df_p['mae'].mean()),\n",
    "                'mape': float(df_p['mape'].mean()),\n",
    "                'coverage': float(df_p['coverage'].mean())\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed for params {params}: {e}\")\n",
    "\n",
    "    res = pd.DataFrame(results).sort_values('rmse')\n",
    "    if res.empty:\n",
    "        raise RuntimeError(\"No tuning results; check data length / CV windows.\")\n",
    "    \n",
    "    best_params = res.iloc[0][list(param_grid.keys())].to_dict()\n",
    "    print(\"\\n‚úÖ Best parameters (by RMSE):\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "    print(f\"   Best RMSE: {res.iloc[0]['rmse']:.2f}\")\n",
    "    print(f\"   Best MAE: {res.iloc[0]['mae']:.2f}\")\n",
    "    print(f\"   Best MAPE: {res.iloc[0]['mape']:.2%}\")\n",
    "    \n",
    "    best_kwargs = dict(base_kwargs)\n",
    "    best_kwargs.update(best_params)\n",
    "    \n",
    "    # Return best parameters only (model will be trained in next cell)\n",
    "    return best_kwargs, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5696391",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING PROPHET MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# CALIBRATED PARAMETERS - Optimized defaults based on tuning results\n",
    "# Note: These will be overridden if RUN_TUNING=True\n",
    "base_kwargs = dict(\n",
    "    growth='linear',                    # Linear for unbounded web traffic\n",
    "    changepoint_prior_scale=0.05,       # Moderate trend flexibility (tuning optimal)\n",
    "    seasonality_prior_scale=0.01,       # Low seasonality strength (tuning optimal)\n",
    "    seasonality_mode='multiplicative',  # Percentage-based patterns (tuning optimal)\n",
    "    daily_seasonality=True,             # Essential for hourly data\n",
    "    weekly_seasonality=True,            # Essential for business patterns\n",
    "    yearly_seasonality=True,            # Enabled with 12 months data\n",
    "    interval_width=0.90                 # 90% confidence intervals\n",
    ")\n",
    "\n",
    "# No cap/floor needed for linear growth\n",
    "if RUN_TUNING:\n",
    "    print(\"\\nüß™ Running hyperparameter tuning...\")\n",
    "    print(\"   ‚ö†Ô∏è  This may take 5-15 minutes (testing 50 parameter combinations)\")\n",
    "    print(\"   Progress bars show Prophet's internal MCMC sampling for each combination\\n\")\n",
    "    \n",
    "    best_params, tuning_results = tune_prophet(\n",
    "        df_train,\n",
    "        param_grid=None,\n",
    "        initial='90 days',      # 25% of 12 months training data\n",
    "        period='30 days',       # Monthly validation cutoffs\n",
    "        horizon='7 days',      # Match FORECAST_DAYS config\n",
    "        base_kwargs=base_kwargs\n",
    "    )\n",
    "    \n",
    "    # Update base_kwargs with best parameters from tuning\n",
    "    base_kwargs.update(best_params)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Updated parameters after tuning:\")\n",
    "    print(f\"   Changepoint prior: {best_params['changepoint_prior_scale']}\")\n",
    "    print(f\"   Seasonality prior: {best_params['seasonality_prior_scale']}\")\n",
    "    print(f\"   Seasonality mode: {best_params['seasonality_mode']}\")\n",
    "    \n",
    "    # Display top 10 results\n",
    "    print(\"\\nüìä Tuning Summary (Top 10 by RMSE):\")\n",
    "    cols = ['changepoint_prior_scale', 'seasonality_prior_scale', 'seasonality_mode', \n",
    "            'rmse', 'mae', 'mape', 'coverage']\n",
    "    print(tuning_results[cols].head(10).to_string(index=False))\n",
    "\n",
    "# Train final model with best parameters (or defaults if tuning disabled)\n",
    "print(\"\\nüìä Training final Prophet model...\")\n",
    "model = Prophet(**base_kwargs)\n",
    "model.fit(df_train)\n",
    "\n",
    "print(\"\\n‚úÖ Prophet model trained successfully!\")\n",
    "\n",
    "# Show model parameters\n",
    "print(\"\\nüìã Model Configuration:\")\n",
    "print(f\"   Growth: {model.growth}\")\n",
    "print(f\"   Changepoint prior scale: {model.changepoint_prior_scale}\")\n",
    "print(f\"   Seasonality prior scale: {model.seasonality_prior_scale}\")\n",
    "print(f\"   Seasonality mode: {model.seasonality_mode}\")\n",
    "print(f\"   Daily seasonality: {model.daily_seasonality}\")\n",
    "print(f\"   Weekly seasonality: {model.weekly_seasonality}\")\n",
    "print(f\"   Yearly seasonality: {model.yearly_seasonality}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ PARAMETER CALIBRATION EXPLAINED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ACTUAL MODEL CONFIGURATION (as trained):\n",
    "   Growth: {model.growth}\n",
    "   Seasonality mode: {model.seasonality_mode}\n",
    "   Changepoint prior: {model.changepoint_prior_scale}\n",
    "   Seasonality prior: {model.seasonality_prior_scale}\n",
    "\n",
    "1Ô∏è‚É£ GROWTH MODEL: Linear \n",
    "   Why: Web traffic has NO UPPER BOUND (can grow indefinitely)\n",
    "   ‚úÖ Linear: Trend continues unbounded ‚Üí y = intercept + slope*t\n",
    "   \n",
    "   Use Linear when:\n",
    "   - Growth can continue indefinitely (web traffic, cloud usage)\n",
    "   - No natural ceiling exists\n",
    "   - Early growth stage (pre-saturation)\n",
    "\n",
    "2Ô∏è‚É£ SEASONALITY MODE: {model.seasonality_mode.upper()}\n",
    "   This was {\"selected by tuning\" if RUN_TUNING else \"set as default\"}\n",
    "   \n",
    "   ‚Ä¢ Additive: Seasonality = fixed amount (e.g., +50 sessions at peak)\n",
    "     Best for: Sparse data, many zeros, constant seasonal patterns\n",
    "   \n",
    "   ‚Ä¢ Multiplicative: Seasonality = percentage (e.g., +20% at peak)\n",
    "     Best for: Dense data, seasonal patterns scale with level\n",
    "\n",
    "3Ô∏è‚É£ CHANGEPOINT_PRIOR_SCALE: {model.changepoint_prior_scale}\n",
    "   Controls trend flexibility\n",
    "   - Lower (0.001-0.01): Rigid trend ‚Üí underfits\n",
    "   - Moderate (0.05-0.1): Balanced ‚Üí recommended\n",
    "   - Higher (0.5+): Flexible trend ‚Üí may overfit\n",
    "   \n",
    "4Ô∏è‚É£ SEASONALITY_PRIOR_SCALE: {model.seasonality_prior_scale}\n",
    "   Controls seasonal pattern strength\n",
    "   - Lower (0.01-0.1): Weak seasonality\n",
    "   - Moderate (1.0-10.0): Normal seasonality\n",
    "   - Higher (20.0+): Strong seasonality\n",
    "\n",
    "5Ô∏è‚É£ YEARLY_SEASONALITY: Enabled (12 months data)\n",
    "   Captures annual patterns (holidays, seasonal trends)\n",
    "   Requires ‚â•12 months data (2+ years ideal)\n",
    "\n",
    "‚úÖ VALIDATION STRATEGY:\n",
    "   - 80/20 holdout split (chronological)\n",
    "   - Multiple metrics (RMSE, MAE, MAPE, R¬≤, coverage, bias)\n",
    "   - Residual analysis (Q-Q plot, heteroscedasticity check)\n",
    "   - Cross-validation with tuning (RUN_TUNING=True) tests 50 combinations\n",
    "\n",
    "üéØ RESULT: Production-ready forecasting system\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26786362",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Holdout Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ac52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HOLDOUT VALIDATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nü§ñ Model Used for Validation:\")\n",
    "print(f\"   Seasonality mode: {model.seasonality_mode}\")\n",
    "print(f\"   Changepoint prior: {model.changepoint_prior_scale}\")\n",
    "print(f\"   Seasonality prior: {model.seasonality_prior_scale}\")\n",
    "\n",
    "# Prepare test set\n",
    "df_test_pred = df_test.copy()\n",
    "\n",
    "# Predict on test set\n",
    "forecast_test = model.predict(df_test_pred)\n",
    "forecast_test['yhat'] = forecast_test['yhat'].clip(lower=0)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = df_test['y'].values\n",
    "y_pred = forecast_test['yhat'].values\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1, y_true))) * 100\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "bias = np.mean(y_pred - y_true)\n",
    "\n",
    "# Coverage\n",
    "in_interval = ((y_true >= forecast_test['yhat_lower'].values) & \n",
    "               (y_true <= forecast_test['yhat_upper'].values))\n",
    "coverage = in_interval.mean()\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "print(f\"   RMSE:     {rmse:.2f} sessions/hour\")\n",
    "print(f\"   MAE:      {mae:.2f} sessions/hour\")\n",
    "print(f\"   MAPE:     {mape:.2f}%\")\n",
    "print(f\"   R¬≤:       {r2:.3f}\")\n",
    "print(f\"   Bias:     {bias:.2f} sessions/hour (should be ~0)\")\n",
    "print(f\"   Coverage: {coverage:.1%} (target: 90%)\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìà Performance Interpretation:\")\n",
    "if mape < 10:\n",
    "    print(\"   ‚úÖ Excellent: MAPE < 10%\")\n",
    "elif mape < 20:\n",
    "    print(\"   ‚úÖ Good: MAPE 10-20%\")\n",
    "elif mape < 30:\n",
    "    print(\"   ‚ö†Ô∏è  Fair: MAPE 20-30%\")\n",
    "else:\n",
    "    print(\"   ‚ùå Poor: MAPE > 30%\")\n",
    "\n",
    "if 0.85 <= coverage <= 0.95:\n",
    "    print(\"   ‚úÖ Coverage within expected range (85-95%)\")\n",
    "elif coverage < 0.85:\n",
    "    print(\"   ‚ö†Ô∏è  Coverage too low - model underestimates uncertainty\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Coverage too high - model overestimates uncertainty\")\n",
    "\n",
    "if abs(bias) < mae * 0.1:\n",
    "    print(\"   ‚úÖ Low bias - model is well-calibrated\")\n",
    "else:\n",
    "    if bias > 0:\n",
    "        print(\"   ‚ö†Ô∏è  Positive bias - model tends to overpredict\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Negative bias - model tends to underpredict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c136418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DIAGNOSTIC: Check data characteristics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Training Data Statistics:\")\n",
    "print(f\"   Mean: {df_train['y'].mean():.2f}\")\n",
    "print(f\"   Median: {df_train['y'].median():.2f}\")\n",
    "print(f\"   Min: {df_train['y'].min():.2f}\")\n",
    "print(f\"   Max: {df_train['y'].max():.2f}\")\n",
    "print(f\"   Std: {df_train['y'].std():.2f}\")\n",
    "print(f\"   Zeros: {(df_train['y'] == 0).sum()} ({(df_train['y'] == 0).mean():.1%})\")\n",
    "\n",
    "print(\"\\nüìä Test Data Statistics:\")\n",
    "print(f\"   Mean: {df_test['y'].mean():.2f}\")\n",
    "print(f\"   Median: {df_test['y'].median():.2f}\")\n",
    "print(f\"   Min: {df_test['y'].min():.2f}\")\n",
    "print(f\"   Max: {df_test['y'].max():.2f}\")\n",
    "\n",
    "print(\"\\nüîç Sample predictions vs actuals (first 24 hours):\")\n",
    "sample = forecast_test[['ds', 'yhat']].head(24).copy()\n",
    "sample['actual'] = df_test['y'].values[:24]\n",
    "sample['error'] = sample['yhat'] - sample['actual']\n",
    "print(sample[['ds', 'actual', 'yhat', 'error']].to_string(index=False))\n",
    "\n",
    "print(\"\\n DIAGNOSIS:\")\n",
    "if df_train['y'].mean() < 10:\n",
    "    print(\"   ‚ùå Very sparse data (mean < 10) - hourly aggregation may be too granular\")\n",
    "    print(\"   üí° Recommendation: Aggregate to daily level instead of hourly\")\n",
    "elif (df_train['y'] == 0).mean() > 0.3:\n",
    "    print(\"   ‚ùå High zero-inflation (>30%) - multiplicative seasonality will fail\")\n",
    "    print(\"   üí° Recommendation: Switch to additive seasonality\")\n",
    "elif df_train['y'].std() / df_train['y'].mean() > 2:\n",
    "    print(\"   ‚ö†Ô∏è  Very high variance - data may need log transformation\")\n",
    "    print(\"   üí° Recommendation: Use log(y+1) transformation\")\n",
    "else: \n",
    "    print(\"Parameters in range. The current setup is reflecting the dataset characteristics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371db0e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Forecast Next 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dce649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORECASTING NEXT 7 DAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Retrain on full dataset for production forecast\n",
    "print(\"\\nüîÑ Retraining on full dataset for production forecast...\")\n",
    "df_full = df_prophet.copy()\n",
    "\n",
    "model_full = Prophet(**base_kwargs).fit(df_full)\n",
    "\n",
    "# Generate forecast (no cap/floor for linear growth)\n",
    "future = model_full.make_future_dataframe(periods=FORECAST_HOURS, freq='H')\n",
    "\n",
    "forecast = model_full.predict(future)\n",
    "print(\"‚úÖ Forecast generated.\")\n",
    "\n",
    "# Clip negatives\n",
    "forecast['yhat'] = forecast['yhat'].clip(lower=0)\n",
    "forecast['yhat_lower'] = forecast['yhat_lower'].clip(lower=0)\n",
    "forecast['yhat_upper'] = forecast['yhat_upper'].clip(lower=0)\n",
    "\n",
    "# Extract future forecast\n",
    "forecast_future = forecast.tail(FORECAST_HOURS).copy()\n",
    "forecast_future['weekday'] = forecast_future['ds'].dt.day_name()\n",
    "forecast_future['hour'] = forecast_future['ds'].dt.hour\n",
    "\n",
    "# IMPROVED: Aggregate using MEAN (not first)\n",
    "prophet_agg = (\n",
    "    forecast_future\n",
    "    .groupby(['weekday','hour'], as_index=False)\n",
    "    .agg({\n",
    "        'yhat': 'mean',\n",
    "        'yhat_lower': 'mean',\n",
    "        'yhat_upper': 'mean'\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'yhat': 'prophet_mean',\n",
    "        'yhat_lower': 'prophet_lower',\n",
    "        'yhat_upper': 'prophet_upper'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Add standard deviation for uncertainty\n",
    "prophet_std = (\n",
    "    forecast_future\n",
    "    .groupby(['weekday','hour'], as_index=False)['yhat']\n",
    "    .std()\n",
    "    .rename(columns={'yhat': 'prophet_std'})\n",
    ")\n",
    "prophet_agg = prophet_agg.merge(prophet_std, on=['weekday', 'hour'])\n",
    "\n",
    "prophet_agg['weekday'] = pd.Categorical(prophet_agg['weekday'], categories=weekday_order, ordered=True)\n",
    "prophet_agg = prophet_agg.sort_values(['weekday','hour']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nüìä Summary for next {FORECAST_DAYS} days (representative week):\")\n",
    "print(f\"   Avg hourly forecast: {prophet_agg['prophet_mean'].mean():.2f} sessions/hour\")\n",
    "print(f\"   Avg uncertainty (std): {prophet_agg['prophet_std'].mean():.2f} sessions/hour\")\n",
    "\n",
    "daily_totals = prophet_agg.groupby('weekday')['prophet_mean'].sum().reindex(weekday_order)\n",
    "print(\"\\n   Daily totals by weekday:\")\n",
    "for day, total in daily_totals.items():\n",
    "    print(f\"      {day}: {total:.1f} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a90ea4",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualizations (Static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATIONS (STATIC)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Heatmaps: Prophet mean and CI width\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
    "\n",
    "heatmap_mean = prophet_agg.pivot(index='hour', columns='weekday', values='prophet_mean').reindex(columns=weekday_order)\n",
    "sns.heatmap(heatmap_mean, cmap='YlOrRd', annot=True, fmt='.1f',\n",
    "            cbar_kws={'label': 'Sessions/Hour'}, linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Prophet Forecast (Hourly Mean)\", fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Weekday\"); axes[0].set_ylabel(\"Hour (UTC)\")\n",
    "\n",
    "heatmap_ci = (prophet_agg.assign(ci_width=lambda d: d['prophet_upper'] - d['prophet_lower'])\n",
    "              .pivot(index='hour', columns='weekday', values='ci_width')\n",
    "              .reindex(columns=weekday_order))\n",
    "sns.heatmap(heatmap_ci, cmap='PuBuGn', annot=True, fmt='.1f',\n",
    "            cbar_kws={'label': 'CI Width (90%)'}, linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Prophet 90% CI Width (Hourly)\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Weekday\"); axes[1].set_ylabel(\"Hour (UTC)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prophet_heatmaps_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); plt.close()\n",
    "\n",
    "# Per-weekday line charts with CI fill\n",
    "fig, axes = plt.subplots(4, 2, figsize=(18, 24)); axes = axes.flatten()\n",
    "for idx, wd in enumerate(weekday_order):\n",
    "    ax = axes[idx]\n",
    "    sub = prophet_agg[prophet_agg['weekday'] == wd].sort_values('hour')\n",
    "    ax.fill_between(sub['hour'], sub['prophet_lower'], sub['prophet_upper'], \n",
    "                     alpha=0.2, color='purple', label='Prophet 90% CI')\n",
    "    ax.plot(sub['hour'], sub['prophet_mean'], 'o-', color='purple', \n",
    "            linewidth=2.5, markersize=6, label='Prophet Forecast')\n",
    "    ax.set_title(f'{wd} ‚Äî Prophet Forecast', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Hour (UTC)'); ax.set_ylabel('Sessions/Hour')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "    ax.set_xticks(range(0,24,2)); ax.set_xlim(-0.5, 23.5)\n",
    "axes[7].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('prophet_weekday_lines_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); plt.close()\n",
    "\n",
    "# Daily totals bar chart\n",
    "x = np.arange(len(weekday_order)); width = 0.6\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(x, daily_totals.values, width, color='purple', alpha=0.85, \n",
    "              edgecolor='black', linewidth=1, label='Prophet')\n",
    "ax.set_xticks(x); ax.set_xticklabels(weekday_order, fontsize=11)\n",
    "ax.set_ylabel('Total Daily Sessions'); ax.set_title('Daily Totals ‚Äî Prophet Forecast', \n",
    "                                                    fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--'); ax.legend()\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + max(daily_totals)*0.01, \n",
    "            f'{h:.0f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('prophet_daily_totals_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); plt.close()\n",
    "\n",
    "print(\"\\n‚úÖ Static visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b3b54",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Prophet Components & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROPHET COMPONENTS & DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Components plot\n",
    "fig_components = model_full.plot_components(forecast)\n",
    "plt.savefig('prophet_components_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); plt.close()\n",
    "\n",
    "# Diagnostics on training set\n",
    "print(\"\\nüìä Creating model diagnostics summary plot...\")\n",
    "_in_sample_fc = model_full.predict(df_full[['ds']])\n",
    "_in = df_full.merge(_in_sample_fc[['ds','yhat','yhat_lower','yhat_upper']], on='ds', how='left')\n",
    "_in['residual'] = _in['y'] - _in['yhat']\n",
    "coverage_train = float(((_in['y'] >= _in['yhat_lower']) & (_in['y'] <= _in['yhat_upper'])).mean())\n",
    "print(f\"   Training set coverage @90%: {coverage_train:.3f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1) Residuals over time\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.plot(_in['ds'], _in['residual'], linewidth=1, alpha=0.6)\n",
    "ax1.axhline(0, color='red', linewidth=2, linestyle='--')\n",
    "ax1.set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Date'); ax1.set_ylabel('Residual (y - yhat)')\n",
    "ax1.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# 2) Residuals vs Fitted\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.scatter(_in['yhat'], _in['residual'], s=10, alpha=0.4)\n",
    "ax2.axhline(0, color='red', linewidth=2, linestyle='--')\n",
    "ax2.set_title('Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Fitted (yhat)'); ax2.set_ylabel('Residual')\n",
    "ax2.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# 3) Histogram of Residuals\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "ax3.hist(_in['residual'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(0, color='red', linewidth=2, linestyle='--')\n",
    "ax3.set_title('Residuals Histogram', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Residual'); ax3.set_ylabel('Frequency')\n",
    "ax3.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# 4) Q-Q Plot\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "(_osm, _osr), (_slope, _intercept, _r) = _stats.probplot(_in['residual'].dropna(), dist=\"norm\")\n",
    "ax4.scatter(_osm, _osr, s=10, alpha=0.6)\n",
    "ax4.plot(_osm, _slope*_osm + _intercept, 'r-', linewidth=2)\n",
    "ax4.set_title('Q-Q Plot of Residuals (Normal)', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Theoretical Quantiles'); ax4.set_ylabel('Ordered Residuals')\n",
    "ax4.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prophet_diagnostics_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); plt.close()\n",
    "\n",
    "print(\"\\n‚úÖ Diagnostics plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf94d0c",
   "metadata": {},
   "source": [
    "## üîü Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   Training period: {TRAINING_MONTHS} months\")\n",
    "print(f\"   Total hours: {len(df_prophet)}\")\n",
    "print(f\"   Date range: {df_prophet['ds'].min()} to {df_prophet['ds'].max()}\")\n",
    "print(f\"   Train/Test split: {int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}%\")\n",
    "\n",
    "print(\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"   Growth: {model_full.growth}\")\n",
    "print(f\"   Changepoint prior: {model_full.changepoint_prior_scale}\")\n",
    "print(f\"   Seasonality prior: {model_full.seasonality_prior_scale}\")\n",
    "print(f\"   Seasonality mode: {model_full.seasonality_mode}\")\n",
    "print(f\"   Seasonalities: Daily={model_full.daily_seasonality}, Weekly={model_full.weekly_seasonality}, Yearly={model_full.yearly_seasonality}\")\n",
    "\n",
    "print(\"\\nüìà Performance Metrics (Test Set):\")\n",
    "print(f\"   RMSE: {rmse:.2f} sessions/hour\")\n",
    "print(f\"   MAE: {mae:.2f} sessions/hour\")\n",
    "print(f\"   MAPE: {mape:.2f}%\")\n",
    "print(f\"   R¬≤: {r2:.3f}\")\n",
    "print(f\"   Coverage: {coverage:.1%}\")\n",
    "\n",
    "print(\"\\nüîÆ Forecast Summary (Next {} Days):\".format(FORECAST_DAYS))\n",
    "print(f\"   Avg hourly: {prophet_agg['prophet_mean'].mean():.2f} sessions/hour\")\n",
    "print(f\"   Total period: {prophet_agg['prophet_mean'].sum():.0f} sessions\")\n",
    "print(f\"   Peak hour: {prophet_agg['prophet_mean'].max():.2f} sessions/hour\")\n",
    "print(f\"   Min hour: {prophet_agg['prophet_mean'].min():.2f} sessions/hour\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã View Tuning Results & Best Model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if tuning was performed\n",
    "if RUN_TUNING and 'tuning_results' in locals():\n",
    "    print(\"\\n‚úÖ Tuning results available!\")\n",
    "    print(\"\\nüìä Top 10 Models by RMSE:\")\n",
    "    cols = ['changepoint_prior_scale', 'seasonality_prior_scale', 'seasonality_mode', \n",
    "            'rmse', 'mae', 'mape', 'coverage']\n",
    "    print(tuning_results[cols].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Models by MAPE:\")\n",
    "    print(tuning_results[cols].sort_values('mape').head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Tuning was not run (RUN_TUNING=False)\")\n",
    "    print(\"   Using default parameters shown below.\")\n",
    "    print(\"   To enable tuning: Set RUN_TUNING=True in configuration cell and re-run.\")\n",
    "\n",
    "# Current model parameters (always available)\n",
    "print(\"\\nü§ñ Current Model Parameters:\")\n",
    "print(f\"   Growth: {model_full.growth}\")\n",
    "print(f\"   Changepoint Prior Scale: {model_full.changepoint_prior_scale}\")\n",
    "print(f\"   Seasonality Prior Scale: {model_full.seasonality_prior_scale}\")\n",
    "print(f\"   Seasonality Mode: {model_full.seasonality_mode}\")\n",
    "print(f\"   Daily Seasonality: {model_full.daily_seasonality}\")\n",
    "print(f\"   Weekly Seasonality: {model_full.weekly_seasonality}\")\n",
    "print(f\"   Yearly Seasonality: {model_full.yearly_seasonality}\")\n",
    "print(f\"   Interval Width: {model_full.interval_width}\")\n",
    "\n",
    "# Model structure\n",
    "print(\"\\nüìà Model Structure:\")\n",
    "print(f\"   Changepoints detected: {len(model_full.changepoints)}\")\n",
    "if len(model_full.changepoints) > 0:\n",
    "    print(f\"   First changepoint: {model_full.changepoints.iloc[0] if hasattr(model_full.changepoints, 'iloc') else model_full.changepoints[0]}\")\n",
    "    print(f\"   Last changepoint: {model_full.changepoints.iloc[-1] if hasattr(model_full.changepoints, 'iloc') else model_full.changepoints[-1]}\")\n",
    "print(f\"   Training observations: {len(df_full)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model information displayed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef04d5",
   "metadata": {},
   "source": [
    "## üé® Interactive Visualizations (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE VISUALIZATIONS (PLOTLY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Interactive Time Series with Forecast\n",
    "print(\"\\nüìä Creating interactive forecast plot...\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "historical = df_full.copy()\n",
    "historical['type'] = 'Historical'\n",
    "\n",
    "forecast_plot = forecast_future.copy()\n",
    "forecast_plot['type'] = 'Forecast'\n",
    "forecast_plot = forecast_plot.rename(columns={'yhat': 'y'})\n",
    "\n",
    "# Create figure\n",
    "fig1 = go.Figure()\n",
    "\n",
    "# Historical data\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=historical['ds'],\n",
    "    y=historical['y'],\n",
    "    mode='lines',\n",
    "    name='Historical Data',\n",
    "    line=dict(color='#1f77b4', width=2),\n",
    "    hovertemplate='<b>Date:</b> %{x}<br><b>Sessions:</b> %{y:.1f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Forecast line\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=forecast_plot['ds'],\n",
    "    y=forecast_plot['y'],\n",
    "    mode='lines',\n",
    "    name='Forecast',\n",
    "    line=dict(color='#ff7f0e', width=3, dash='dash'),\n",
    "    hovertemplate='<b>Date:</b> %{x}<br><b>Forecast:</b> %{y:.1f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Confidence interval\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=forecast_plot['ds'],\n",
    "    y=forecast_plot['yhat_upper'],\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=forecast_plot['ds'],\n",
    "    y=forecast_plot['yhat_lower'],\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    fillcolor='rgba(255, 127, 14, 0.2)',\n",
    "    fill='tonexty',\n",
    "    name='90% Confidence Interval',\n",
    "    hovertemplate='<b>Date:</b> %{x}<br><b>Lower:</b> %{y:.1f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    title=f'Hourly Web Traffic Forecast ({FORECAST_DAYS} Days Ahead)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sessions/Hour',\n",
    "    hovermode='x unified',\n",
    "    height=500,\n",
    "    template='plotly_white',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# 2. Interactive Heatmap - Forecast by Day/Hour\n",
    "print(\"\\nüìä Creating interactive heatmap...\")\n",
    "\n",
    "heatmap_data = prophet_agg.pivot(index='hour', columns='weekday', values='prophet_mean').reindex(columns=weekday_order)\n",
    "\n",
    "fig2 = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data.values,\n",
    "    x=heatmap_data.columns,\n",
    "    y=heatmap_data.index,\n",
    "    colorscale='YlOrRd',\n",
    "    hovertemplate='<b>%{x}</b><br>Hour: %{y}<br>Sessions: %{z:.1f}<extra></extra>',\n",
    "    colorbar=dict(title='Sessions/Hour')\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Forecast Heatmap: Sessions by Day & Hour',\n",
    "    xaxis_title='Day of Week',\n",
    "    yaxis_title='Hour (UTC)',\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig2.update_yaxes(autorange='reversed')\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "# 3. Interactive Daily Totals Bar Chart\n",
    "print(\"\\nüìä Creating interactive bar chart...\")\n",
    "\n",
    "daily_totals_df = pd.DataFrame({\n",
    "    'weekday': weekday_order,\n",
    "    'total': daily_totals.reindex(weekday_order).values\n",
    "})\n",
    "\n",
    "fig3 = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=daily_totals_df['weekday'],\n",
    "        y=daily_totals_df['total'],\n",
    "        marker_color='purple',\n",
    "        marker_line_color='black',\n",
    "        marker_line_width=1.5,\n",
    "        hovertemplate='<b>%{x}</b><br>Total Sessions: %{y:.0f}<extra></extra>',\n",
    "        text=daily_totals_df['total'].round(0),\n",
    "        textposition='outside',\n",
    "        texttemplate='%{text:.0f}'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='Daily Total Sessions by Weekday (Forecast)',\n",
    "    xaxis_title='Day of Week',\n",
    "    yaxis_title='Total Daily Sessions',\n",
    "    height=500,\n",
    "    template='plotly_white',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "# 4. Interactive Components Decomposition (Weekly, Yearly, Daily)\n",
    "print(\"\\nüìä Creating interactive components plot...\")\n",
    "\n",
    "# Extract components from forecast\n",
    "components_df = forecast[['ds', 'weekly', 'yearly', 'daily']].tail(FORECAST_HOURS * 2)  # Last 2x forecast period\n",
    "\n",
    "fig4 = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=('Weekly Seasonality', 'Yearly Seasonality', 'Daily Seasonality'),\n",
    "    vertical_spacing=0.1,\n",
    "    row_heights=[0.33, 0.33, 0.33]\n",
    ")\n",
    "\n",
    "# Weekly\n",
    "fig4.add_trace(\n",
    "    go.Scatter(x=components_df['ds'], y=components_df['weekly'], \n",
    "               mode='lines', name='Weekly', line=dict(color='green', width=2),\n",
    "               hovertemplate='<b>Date:</b> %{x}<br><b>Weekly:</b> %{y:.1f}<extra></extra>'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Yearly\n",
    "fig4.add_trace(\n",
    "    go.Scatter(x=components_df['ds'], y=components_df['yearly'], \n",
    "               mode='lines', name='Yearly', line=dict(color='red', width=2),\n",
    "               hovertemplate='<b>Date:</b> %{x}<br><b>Yearly:</b> %{y:.1f}<extra></extra>'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Daily\n",
    "fig4.add_trace(\n",
    "    go.Scatter(x=components_df['ds'], y=components_df['daily'], \n",
    "               mode='lines', name='Daily', line=dict(color='purple', width=2),\n",
    "               hovertemplate='<b>Date:</b> %{x}<br><b>Daily:</b> %{y:.1f}<extra></extra>'),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig4.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "fig4.update_yaxes(title_text=\"Effect\", row=1, col=1)\n",
    "fig4.update_yaxes(title_text=\"Effect\", row=2, col=1)\n",
    "fig4.update_yaxes(title_text=\"Effect\", row=3, col=1)\n",
    "\n",
    "fig4.update_layout(\n",
    "    title_text='Forecast Components Decomposition',\n",
    "    height=800,\n",
    "    template='plotly_white',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "# 5. Interactive Residuals Analysis (if test data available)\n",
    "print(\"\\nüìä Creating interactive residuals plot...\")\n",
    "\n",
    "if 'df_test' in locals() and 'forecast_test' in locals() and len(df_test) > 0:\n",
    "    residuals_df = pd.DataFrame({\n",
    "        'ds': df_test['ds'].values,\n",
    "        'actual': df_test['y'].values,\n",
    "        'predicted': forecast_test['yhat'].values,\n",
    "        'residual': df_test['y'].values - forecast_test['yhat'].values\n",
    "    })\n",
    "    \n",
    "    fig5 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Residuals Over Time', 'Residuals vs Predicted', \n",
    "                        'Residual Distribution', 'Actual vs Predicted'),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Residuals over time\n",
    "    fig5.add_trace(\n",
    "        go.Scatter(x=residuals_df['ds'], y=residuals_df['residual'],\n",
    "                   mode='markers', name='Residuals',\n",
    "                   marker=dict(size=4, color='blue', opacity=0.6),\n",
    "                   hovertemplate='<b>Date:</b> %{x}<br><b>Residual:</b> %{y:.1f}<extra></extra>'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig5.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=1)\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    fig5.add_trace(\n",
    "        go.Scatter(x=residuals_df['predicted'], y=residuals_df['residual'],\n",
    "                   mode='markers', name='Residuals',\n",
    "                   marker=dict(size=4, color='green', opacity=0.6),\n",
    "                   hovertemplate='<b>Predicted:</b> %{x:.1f}<br><b>Residual:</b> %{y:.1f}<extra></extra>'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig5.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=2)\n",
    "    \n",
    "    # Residual distribution\n",
    "    fig5.add_trace(\n",
    "        go.Histogram(x=residuals_df['residual'], nbinsx=30,\n",
    "                     marker_color='purple', opacity=0.7,\n",
    "                     name='Distribution',\n",
    "                     hovertemplate='<b>Residual Range:</b> %{x}<br><b>Count:</b> %{y}<extra></extra>'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Actual vs Predicted\n",
    "    fig5.add_trace(\n",
    "        go.Scatter(x=residuals_df['actual'], y=residuals_df['predicted'],\n",
    "                   mode='markers', name='Predictions',\n",
    "                   marker=dict(size=4, color='orange', opacity=0.6),\n",
    "                   hovertemplate='<b>Actual:</b> %{x:.1f}<br><b>Predicted:</b> %{y:.1f}<extra></extra>'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    # Perfect prediction line\n",
    "    min_val = min(residuals_df['actual'].min(), residuals_df['predicted'].min())\n",
    "    max_val = max(residuals_df['actual'].max(), residuals_df['predicted'].max())\n",
    "    fig5.add_trace(\n",
    "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
    "                   mode='lines', name='Perfect Fit',\n",
    "                   line=dict(color='red', dash='dash', width=2),\n",
    "                   hoverinfo='skip'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig5.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig5.update_xaxes(title_text=\"Predicted Value\", row=1, col=2)\n",
    "    fig5.update_xaxes(title_text=\"Residual\", row=2, col=1)\n",
    "    fig5.update_xaxes(title_text=\"Actual Value\", row=2, col=2)\n",
    "    \n",
    "    fig5.update_yaxes(title_text=\"Residual\", row=1, col=1)\n",
    "    fig5.update_yaxes(title_text=\"Residual\", row=1, col=2)\n",
    "    fig5.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "    fig5.update_yaxes(title_text=\"Predicted Value\", row=2, col=2)\n",
    "    \n",
    "    fig5.update_layout(\n",
    "        title_text='Residuals Analysis - Model Diagnostics',\n",
    "        height=800,\n",
    "        template='plotly_white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig5.show()\n",
    "    print(\"‚úÖ Residuals plot created successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping residuals plot - test data or forecast_test not available\")\n",
    "\n",
    "print(\"\\n‚úÖ All interactive visualizations created successfully!\")\n",
    "print(\"   üí° Tip: Hover over plots for details, click and drag to zoom, double-click to reset\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
